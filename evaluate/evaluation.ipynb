{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime, string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# 모든 원본 데이터를 담은 df를 만든다 (Answer를 일치시키도록 복원하기 위해서)\n",
    "import pandas as pd\n",
    "\n",
    "LogiQA2_val = pd.read_json('/hdd/hjl8708/workspace/Data/LogiQA-2/logiqa-val.jsonl', lines=True)\n",
    "# MMLU_val = pd.read_json('/hdd/hjl8708/workspace/Data/MMLU/MMLU-val.jsonl', lines=True)\n",
    "RULE_mainq = pd.read_json('/hdd/hjl8708/workspace/Data/RULE/RULE_mainq.jsonl', lines=True)\n",
    "\n",
    "# 위 3개의 데이터를 합친다\n",
    "original_data = pd.concat([LogiQA2_val, RULE_mainq], ignore_index=True)\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    max_len = len(ground_truth)\n",
    "    return (normalize_answer(prediction, max_len) == normalize_answer(ground_truth, max_len))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    max_len = len(ground_truth)\n",
    "    prediction_tokens = normalize_answer(prediction,max_len).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth,max_len).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def metric_match(metric, prediction, ground_truth):\n",
    "    score = metric(prediction, ground_truth)\n",
    "    return score\n",
    "\n",
    "# def normalize_answer_fewshot_LLaMA(s):\n",
    "def normalize_answer(s, max_len):\n",
    "    # should we keep those counter removal? \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    # 구현\n",
    "    lowered_pred = remove_punc(s.lower())\n",
    "    # print('original:', s)\n",
    "    if lowered_pred.startswith(\"yes\") or lowered_pred.startswith(\"no\"):\n",
    "        if lowered_pred.startswith(\"yes\"): \n",
    "            extract_s = s[:3]\n",
    "        elif lowered_pred.startswith(\"no\"):\n",
    "            extract_s = s[:2]\n",
    "    else:\n",
    "        extract_s = lowered_pred[0]\n",
    "        \n",
    "    # print('extracted:', extract_s)\n",
    "    return white_space_fix(remove_punc(lower(extract_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "# from utils.tools import read_jsonl, check_jsonls, metric_max_over_ground_truths,  f1_score, exact_match_score, metric_match\n",
    "import pandas as pd \n",
    "\n",
    "original_data_RULE_path = '/hdd/hjl8708/workspace/Data/RULE/RULE_subq_all.jsonl'\n",
    "original_data_RULE = pd.read_json(original_data_RULE_path, lines=True) #: [{'question_id': str, 'prediction': str}\n",
    "\n",
    "def dtype_processing(pred):\n",
    "    dtypes = ['logiqa', 'ReClor_val', 'RULE_train', 'MMLU_val']\n",
    "    sub_types = ['sub', 'option', 'shuffle']\n",
    "    # qid에서 다음 것들 중에 겹치는 부분을 추출한다\n",
    "    \n",
    "    result = []\n",
    "    qid = pred[\"qid\"]\n",
    "    \n",
    "    for dtype in dtypes:\n",
    "        if dtype in qid:\n",
    "            result.append(dtype)\n",
    "    for sub_type in sub_types:\n",
    "        if sub_type in qid:\n",
    "            result.append(sub_type)\n",
    "    \n",
    "    if 'option' in result:\n",
    "        if 'yes' in pred['answer']:\n",
    "            result.append('selective')\n",
    "        else:\n",
    "            result.append('eliminative')\n",
    "    \n",
    "    if 'sub' in result:\n",
    "        def get_main_option_correctness(qid):\n",
    "            row = original_data_RULE[original_data_RULE['id_string'] == qid]\n",
    "            selective = row['others'].item()['main_option_correctness']\n",
    "            return 'selective' if selective else 'eliminative'\n",
    "        result.append(get_main_option_correctness(qid))\n",
    "        \n",
    "    result = '_'.join(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def gen_eval_new(preds):\n",
    "    em_total = 0\n",
    "    f1_total = 0\n",
    "    count = 0\n",
    "    \n",
    "    results = {} # qid에 있는 type마다 em, f1을 저장\n",
    "    \n",
    "    for pred in preds:\n",
    "        qid = pred[\"qid\"]\n",
    "        sent = pred[\"question\"].lower().strip()\n",
    "        count += 1\n",
    "        gold = pred[\"answer\"]\n",
    "        \n",
    "\n",
    "        prediction = pred[\"output\"]\n",
    "\n",
    "        em_current = metric_match(exact_match_score, prediction, gold)\n",
    "        em_total += em_current\n",
    "        \n",
    "        dtype = dtype_processing(pred)\n",
    "        \n",
    "        if dtype in results:\n",
    "            results[dtype]['em'] += em_current\n",
    "            results[dtype]['count'] += 1\n",
    "        else:\n",
    "            results[dtype] = {'em': em_current, 'count': 1}\n",
    "        \n",
    "    for dtype in results:\n",
    "        results[dtype]['em'] /= results[dtype]['count']\n",
    "        results[dtype]['em'] *= 100\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 여기서부터 시작 (결과)\n",
    "\n",
    "dataname = 'Test-AMR_LDA-mixtral_instruct'\n",
    "pred_foler = f'/hdd/hjl8708/workspace/experiments/{dataname}'\n",
    "pred_file: str = f'{pred_foler}/greedy_preds.json'\n",
    "with open(pred_file, 'r') as f:\n",
    "    preds = json.load(f) # {'qid': str, 'question': str, 'output': str, 'answer': str}\n",
    "    \n",
    "# golds = read_jsonl(gold_file)\n",
    "# check_jsonls(preds, golds)\n",
    "\n",
    "results = gen_eval_new(preds)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# # results를 pd DF로\n",
    "df = pd.DataFrame(results).T\n",
    "preds_df = pd.DataFrame(preds)\n",
    "\n",
    "with open(f'{pred_foler}/main_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/943 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:07<00:00, 122.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# 정답 여부와 관계 없이, 선택지에 대해서 같은 판단을 내렸는가에 관한 평가 (정답인 경우와 아닌 경우도 나눠서 확인하기)\n",
    "# preds에서 각 선택지에 대한 판단을 추출\n",
    "    # MCQA type: 정답 배열 중에서 output에 가장 가까운 문장을 선택함\n",
    "    # Option type: 데이터 원본을 확인해서 원본 정답 문장을 확인하고 yes/no를 통해 선택지 판단을 확인\n",
    "\n",
    "def mcqa_answer_extraction(pred: str, answers, normalize_function):\n",
    "    def get_minimum_edit_distance(a, b):\n",
    "        m, n = len(a), len(b)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        # 기저 사례 초기화\n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i  # str1의 첫 i 글자를 공백으로 변환하는 비용\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j  # 공백을 str2의 첫 j 글자로 변환하는 비용\n",
    "        # dp 테이블 채우기\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if a[i-1] == b[j-1]:\n",
    "                    cost = 0  # 글자가 같을 경우, 비용은 0\n",
    "                else:\n",
    "                    cost = 1  # 글자가 다를 경우, 비용은 1\n",
    "                dp[i][j] = min(\n",
    "                    dp[i-1][j] + 1,  # 삭제\n",
    "                    dp[i][j-1] + 1,  # 삽입\n",
    "                    dp[i-1][j-1] + cost  # 교체 또는 매칭\n",
    "                )\n",
    "        # 우측 하단에 있는 값이 최소 편집 거리\n",
    "        return dp[m][n]    \n",
    "    minimum_edit_distance = []\n",
    "    \n",
    "    \n",
    "    pred_norm = normalize_function(pred, max(len(answer) for answer in answers))\n",
    "    \n",
    "    for answer in answers:\n",
    "        minimum_edit_distance.append(get_minimum_edit_distance(pred_norm, normalize_function(answer, len(answer))))\n",
    "    \n",
    "    return answers[minimum_edit_distance.index(min(minimum_edit_distance))]\n",
    "\n",
    "\n",
    "# 1. main_question의 답변을 추출한다\n",
    "mainq_qids = [] # 대상이 되는 main question들의 qid\n",
    "for pred in preds:\n",
    "    if 'option' not in pred['qid'] and 'shuffle' not in pred['qid'] and 'sub' not in pred['qid']:\n",
    "        mainq_qids.append(pred['qid'])\n",
    "\n",
    "# 중복되는 경우 2가지 (From LogiQA): 스킵\n",
    "result_for_mainq = []\n",
    "from tqdm import tqdm \n",
    "for main_qid in tqdm(mainq_qids):\n",
    "    tmp = {}\n",
    "    tmp['qid'] = main_qid\n",
    "    \n",
    "    # qid가 original_data에서 두 개 잡히는 경우 그 무시하기\n",
    "    if len(original_data[original_data['id_string'] == main_qid]) > 1:\n",
    "        print(f\"qid가 original_data에서 두 개 잡히는 경우: {main_qid}\")\n",
    "        continue\n",
    "    # answer_options의 모든 원소에 str() 적용\n",
    "    answer_options = original_data[original_data['id_string'] == main_qid]['answers'].item()\n",
    "    answer_options = [str(answer) for answer in answer_options]\n",
    "    output = preds_df[preds_df['qid'] == main_qid]['output'].item()\n",
    "    tmp['main_result'] = mcqa_answer_extraction(output, answer_options, normalize_answer)\n",
    "    tmp['answer'] = preds_df[preds_df['qid'] == main_qid]['answer'].item()\n",
    "    result_for_mainq.append(tmp)\n",
    "    \n",
    "result_for_mains = f'{pred_foler}/result_for_mains.json'\n",
    "\n",
    "data_ids = ['RULE_train', 'logiqa', 'MMLU_val']\n",
    "\n",
    "for i in result_for_mainq: # 데이터 타입을 마킹\n",
    "    for data_id in data_ids:\n",
    "        if data_id in i['qid']:\n",
    "            i['data_id'] = data_id\n",
    "            break\n",
    "\n",
    "with open(result_for_mains, 'w') as f:\n",
    "    json.dump(result_for_mainq, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 534/3003 [00:40<03:06, 13.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m output \u001b[38;5;241m=\u001b[39m preds_df[preds_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m subq_qid][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_output\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m---> 48\u001b[0m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_result\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmcqa_answer_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds_df[preds_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m subq_qid][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     50\u001b[0m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_option_correctness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m main_option_correctness\n",
      "Cell \u001b[0;32mIn[146], line 35\u001b[0m, in \u001b[0;36mmcqa_answer_extraction\u001b[0;34m(pred, answers, normalize_function)\u001b[0m\n\u001b[1;32m     32\u001b[0m pred_norm \u001b[38;5;241m=\u001b[39m normalize_function(pred, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(answer) \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers:\n\u001b[0;32m---> 35\u001b[0m     minimum_edit_distance\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_minimum_edit_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answers[minimum_edit_distance\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmin\u001b[39m(minimum_edit_distance))]\n",
      "Cell \u001b[0;32mIn[146], line 18\u001b[0m, in \u001b[0;36mmcqa_answer_extraction.<locals>.get_minimum_edit_distance\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m a[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m b[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     19\u001b[0m             cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 글자가 같을 경우, 비용은 0\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 여기서부터 시작 (결과)\n",
    "\n",
    "pred_foler = f'/hdd/hjl8708/workspace/experiments/{dataname}'\n",
    "pred_file: str = f'{pred_foler}/greedy_preds.json'\n",
    "with open(pred_file, 'r') as f:\n",
    "    preds = json.load(f) # {'qid': str, 'question': str, 'output': str, 'answer': str}\n",
    "    \n",
    "# golds = read_jsonl(gold_file)\n",
    "# check_jsonls(preds, golds)\n",
    "\n",
    "results = gen_eval_new(preds)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# # results를 pd DF로\n",
    "df = pd.DataFrame(results).T\n",
    "# # df\n",
    "\n",
    "preds_df = pd.DataFrame(preds)\n",
    "\n",
    "# SubQ 답변을 추출한다\n",
    "subq_qids = [] # 대상이 되는 main question들의 qid\n",
    "for pred in preds:\n",
    "    if 'sub' in pred['qid']:\n",
    "        subq_qids.append(pred['qid'])\n",
    "\n",
    "RULE_subq = pd.read_json('/hdd/hjl8708/workspace/Data/RULE/RULE_subq_all.jsonl', lines=True)\n",
    "\n",
    "# 중복되는 경우 2가지 (From LogiQA): 스킵\n",
    "result_for_subq = []\n",
    "from tqdm import tqdm \n",
    "for subq_qid in tqdm(subq_qids):\n",
    "    tmp = {}\n",
    "    tmp['qid'] = subq_qid\n",
    "    \n",
    "    # qid가 RULE_subq에서 두 개 잡히는 경우 그 무시하기\n",
    "    if len(RULE_subq[RULE_subq['id_string'] == subq_qid]) > 1:\n",
    "        print(f\"qid가 RULE_subq에서 두 개 잡히는 경우: {subq_qid}\")\n",
    "        continue\n",
    "    # answer_options의 모든 원소에 str() 적용\n",
    "    answer_options = RULE_subq[RULE_subq['id_string'] == subq_qid]['answers'].item()\n",
    "    main_option_correctness = RULE_subq[RULE_subq['id_string'] == subq_qid]['others'].item()['main_option_correctness']\n",
    "    answer_options = [str(answer) for answer in answer_options]\n",
    "    output = preds_df[preds_df['qid'] == subq_qid]['output'].item()\n",
    "    tmp['original_output'] = output\n",
    "    tmp['main_result'] = mcqa_answer_extraction(output, answer_options, normalize_answer)\n",
    "    tmp['answer'] = preds_df[preds_df['qid'] == subq_qid]['answer'].item()\n",
    "    tmp['main_option_correctness'] = main_option_correctness\n",
    "    result_for_subq.append(tmp)\n",
    "    \n",
    "result_for_subs = f'{pred_foler}/result_for_subqs.json'\n",
    "\n",
    "with open(result_for_subs, 'w') as f:\n",
    "    json.dump(result_for_subq, f, indent=4)\n",
    "    \n",
    "\n",
    "df_subq_result = pd.DataFrame(result_for_subq)\n",
    "df_subq_result['EM'] = df_subq_result['main_result'] == df_subq_result['answer']\n",
    "\n",
    "df_subq_result_selective = df_subq_result[df_subq_result['main_option_correctness'] == True]\n",
    "df_subq_result_selective['EM'] = df_subq_result_selective['main_result'] == df_subq_result_selective['answer']\n",
    "df_subq_result_selective.EM.mean()\n",
    "\n",
    "df_subq_result_eliminative = df_subq_result[df_subq_result['main_option_correctness'] == False]\n",
    "df_subq_result_eliminative['EM'] = df_subq_result_eliminative['main_result'] == df_subq_result_eliminative['answer']\n",
    "df_subq_result_eliminative.EM.mean()\n",
    "\n",
    "print(f\"SubQ EM: {df_subq_result.EM.mean()}, Selective EM: {df_subq_result_selective.EM.mean()}, Eliminative EM: {df_subq_result_eliminative.EM.mean()}\")\n",
    "\n",
    "subq_result_score_path = f'{pred_foler}/subq_result_score.json'\n",
    "subq_result_score = {'SubQ EM': df_subq_result.EM.mean(), 'Selective EM': df_subq_result_selective.EM.mean(), 'Eliminative EM': df_subq_result_eliminative.EM.mean()}\n",
    "with open(subq_result_score_path, 'w') as f:\n",
    "    json.dump(subq_result_score, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option consistency 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4039 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4039/4039 [00:59<00:00, 67.39it/s]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "data_for_mains: [\n",
    "    {\n",
    "        'qid': str,\n",
    "        'main_result': 선택지문장\n",
    "    }\n",
    "]\n",
    "'''\n",
    "result_for_mains = f'{pred_foler}/result_for_mains.json'\n",
    "with open(result_for_mains, 'r') as f:\n",
    "    data_for_mains = json.load(f)\n",
    "\n",
    "def extract_option_from_option_question(query):\n",
    "    # 1. query에서 마지막으로 등장하는 '・'를 찾고 그 뒤만 남긴다\n",
    "    query_1 = query.split('・')[-1]\n",
    "    # 2. 그 뒤에 있는 문장을 \" 앞까지만 남긴다\n",
    "    query_2 = query_1.split('\"')[0].strip()\n",
    "    return query_2\n",
    "\n",
    "def yes_no_from_output(output):\n",
    "    output.replace(\"The answer is:\", \"\")\n",
    "    if 'yes' in output.lower():\n",
    "        return 'yes'\n",
    "    elif 'no' in output.lower():\n",
    "        return 'no'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "        \n",
    "data_ids_count = { # 데이터 타입에 따른 경향성을 보기 위해 카운트\n",
    "    'RULE_train': {\n",
    "                'consistent_count': 0,\n",
    "                'inconsistent_count': 0,\n",
    "                'main_correct': {\n",
    "                    'consistent_count': 0,\n",
    "                    'inconsistent_count': 0\n",
    "                }\n",
    "                },\n",
    "    'logiqa': {\n",
    "                'consistent_count': 0,\n",
    "                'inconsistent_count': 0,\n",
    "                'main_correct': {\n",
    "                    'consistent_count': 0,\n",
    "                    'inconsistent_count': 0\n",
    "                }},\n",
    "    'MMLU_val': {\n",
    "                'consistent_count': 0,\n",
    "                'inconsistent_count': 0,\n",
    "                'main_correct': {\n",
    "                    'consistent_count': 0,\n",
    "                    'inconsistent_count': 0\n",
    "                }},\n",
    "}\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data_for_mains))): \n",
    "    option_result = []\n",
    "    target_main_qid = data_for_mains[i]['qid']\n",
    "    target_option = data_for_mains[i]['main_result']\n",
    "    target_main_correctness = data_for_mains[i]['answer'] == data_for_mains[i]['main_result']\n",
    "    \n",
    "    qid_match_rows = preds_df[preds_df['qid'].str.contains(target_main_qid)]\n",
    "    option_rows = qid_match_rows[qid_match_rows['qid'].str.contains('option')]\n",
    "    \n",
    "    for j in range(len(option_rows)):\n",
    "        option_q_target = extract_option_from_option_question(option_rows.iloc[j]['question'])\n",
    "        option_q_decision = yes_no_from_output(option_rows.iloc[j]['output'])\n",
    "        option_q_decision_label = 'yes' if target_option == option_q_target else 'no'\n",
    "        if option_q_decision == option_q_decision_label:\n",
    "            data_ids_count[data_for_mains[i]['data_id']]['consistent_count'] += 1\n",
    "        else:\n",
    "            data_ids_count[data_for_mains[i]['data_id']]['inconsistent_count'] += 1\n",
    "            \n",
    "        if target_main_correctness:\n",
    "            if option_q_decision == option_q_decision_label:\n",
    "                data_ids_count[data_for_mains[i]['data_id']]['main_correct']['consistent_count'] += 1\n",
    "            else:\n",
    "                data_ids_count[data_for_mains[i]['data_id']]['main_correct']['inconsistent_count'] += 1\n",
    "        \n",
    "# option_consistency_result.json에 저장\n",
    "\n",
    "option_consistency_result_path = f'{pred_foler}/option_consistency_result.json'\n",
    "with open(option_consistency_result_path, 'w') as f:\n",
    "    json.dump(data_ids_count, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수 계산\n",
    "\n",
    "option_consistency_result_path = f'{pred_foler}/option_consistency_result.json'\n",
    "with open(option_consistency_result_path, 'r') as f:\n",
    "    option_consistency_result = json.load(f)    \n",
    "    \n",
    "data_ids_scores = { # 데이터 타입에 따른 경향성을 보기 위해 카운트\n",
    "    'RULE_train': {\n",
    "                'option_consistency_total': 0,\n",
    "                'option_consistency_main_correct': 0,\n",
    "                'option_consistency_main_incorrect': 0,\n",
    "                },\n",
    "    'logiqa': {\n",
    "                'option_consistency_total': 0,\n",
    "                'option_consistency_main_correct': 0,\n",
    "                'option_consistency_main_incorrect': 0,\n",
    "                },\n",
    "    'MMLU_val': {\n",
    "                'option_consistency_total': 0,\n",
    "                'option_consistency_main_correct': 0,\n",
    "                'option_consistency_main_incorrect': 0,\n",
    "                },\n",
    "}\n",
    "\n",
    "for i in data_ids_scores:\n",
    "    total_count = option_consistency_result[i]['consistent_count'] + option_consistency_result[i]['inconsistent_count']\n",
    "    total_consistent = option_consistency_result[i]['consistent_count']\n",
    "    total_inconsistent = option_consistency_result[i]['inconsistent_count']\n",
    "    \n",
    "    correct_consistent = option_consistency_result[i]['main_correct']['consistent_count']\n",
    "    correct_inconsistent = option_consistency_result[i]['main_correct']['inconsistent_count']\n",
    "    total_correct = correct_consistent + correct_inconsistent\n",
    "    \n",
    "    incorrect_consistent = total_consistent - correct_consistent\n",
    "    incorrect_inconsistent = total_inconsistent - correct_inconsistent\n",
    "    total_incorrect = incorrect_consistent + incorrect_inconsistent\n",
    "    \n",
    "    data_ids_scores[i]['option_consistency_total'] =  total_consistent / total_count\n",
    "    data_ids_scores[i]['option_consistency_main_correct'] = correct_consistent / total_correct\n",
    "    data_ids_scores[i]['option_consistency_main_incorrect'] = incorrect_consistent / total_incorrect\n",
    "      \n",
    "# data_ids_scores를 option_consistency_scores.json으로 저장\n",
    "option_consistency_scores_path = f'{pred_foler}/option_consistency_scores.json'\n",
    "with open(option_consistency_scores_path, 'w') as f:\n",
    "    json.dump(data_ids_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4039 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4039/4039 [00:59<00:00, 68.31it/s]\n"
     ]
    }
   ],
   "source": [
    "result_for_mains = f'{pred_foler}/result_for_mains.json'\n",
    "with open(result_for_mains, 'r') as f:\n",
    "    data_for_mains = json.load(f)\n",
    "\n",
    "\n",
    "def remove_prefix(text):\n",
    "    if \"The answer is:\" in text:\n",
    "        text = text.split(\"The answer is:\")[1]\n",
    "        text = text.split(\"\\n\")[0].strip()\n",
    "    else:\n",
    "        text = text\n",
    "    return text \n",
    "    # return text.replace(\"The answer is: \", \"\")\n",
    "\n",
    "data_ids_count = { # 데이터 타입에 따른 경향성을 보기 위해 카운트\n",
    "    'RULE_train': {\n",
    "                'consistency_scores': [],\n",
    "                },\n",
    "    'logiqa': {\n",
    "                'consistency_scores': [],\n",
    "                },\n",
    "    'MMLU_val': {\n",
    "                'consistency_scores': [],\n",
    "                },\n",
    "}\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data_for_mains))): \n",
    "    option_result = []\n",
    "    target_main_qid = data_for_mains[i]['qid']\n",
    "    target_option = data_for_mains[i]['main_result']\n",
    "    target_main_correctness = data_for_mains[i]['answer'] == data_for_mains[i]['main_result']\n",
    "    \n",
    "    qid_match_rows = preds_df[preds_df['qid'].str.contains(target_main_qid)]\n",
    "    shuffle_rows = qid_match_rows[qid_match_rows['qid'].str.contains('shuffle')]\n",
    "    \n",
    "    prediction_list = [target_option]\n",
    "    for j in range(len(shuffle_rows)):\n",
    "        prediction_list.append(normalize_answer(shuffle_rows.iloc[j]['output'], len(target_option)))\n",
    "        \n",
    "    # prediction_list에서 각 값의 등장 횟수를 센다\n",
    "    prediction_count = Counter(prediction_list)\n",
    "    data_ids_count[data_for_mains[i]['data_id']]['consistency_scores'].append(prediction_count[max(prediction_count)])\n",
    "\n",
    "scores = {\n",
    "    'RULE_train': 25*sum(data_ids_count['RULE_train']['consistency_scores']) / len(data_ids_count['RULE_train']['consistency_scores']),\n",
    "    'logiqa': 25*sum(data_ids_count['logiqa']['consistency_scores']) / len(data_ids_count['logiqa']['consistency_scores']),\n",
    "    'MMLU_val': 25*sum(data_ids_count['MMLU_val']['consistency_scores']) / len(data_ids_count['MMLU_val']['consistency_scores']),\n",
    "}\n",
    "\n",
    "# scores를 저장\n",
    "scores_path = f'{pred_foler}/shuffle_consistency_scores.json'\n",
    "with open(scores_path, 'w') as f:\n",
    "    json.dump(scores, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hdd/hjl8708/workspace/EM_evaluation/results/setting3_EM_LoRA_after/greedy_preds.json'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: The answer is: yes.\n",
      "extracted: The answer is: yes.\n",
      "original: yes.\n",
      "extracted: yes.\n",
      "original: The answer is: yes.\n",
      "extracted: The answer is: yes.\n",
      "original: yes.\n",
      "extracted: yes.\n",
      "<Passage>: Trainer: I recently developed an exercise routine that can get anybody to meet his or her goals. The routine combines cardio and bodybuilding during each session for the purpose of losing weight. Every person I' ve trained has lost weight on the program.\n",
      "The strength of the argument depends on which one of the following?\n",
      "・ Every client the trainer has worked with has weight loss as a goal.\n",
      "・ Every client the trainer has worked with has prior experience lifting weights.\n",
      "・ Every client the trainer has worked with has also adopted a healthy diet.\n",
      "・ Losing weight is always a healthy outcome.\n",
      "\n",
      "\n",
      "<Question>: Is the option \"・ Every client the trainer has worked with has weight loss as a goal.\" is a correct answer?\n",
      "pred: The answer is: yes.\n",
      "gold: yes.\n",
      "\n",
      "em: True\n",
      "f1: 1.0\n"
     ]
    }
   ],
   "source": [
    "def pr_tmp(preds, i):\n",
    "    pred = preds[i]\n",
    "    prediction = pred[\"output\"]\n",
    "    gold = pred[\"answer\"]\n",
    "\n",
    "    dtype = dtype_processing(pred)\n",
    "    em_current = metric_match(exact_match_score, prediction, gold)\n",
    "    f1_current = metric_match(f1_score, prediction, gold)\n",
    "\n",
    "    print(f\"\"\"\n",
    "{pred[\"question\"]}\n",
    "pred: {prediction}\n",
    "gold: {gold}\n",
    "\n",
    "em: {em_current}\n",
    "f1: {f1_current}\n",
    "\n",
    "    \"\"\".strip())\n",
    "\n",
    "i = 3003\n",
    "pr_tmp(preds, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 'RULE_train_1_sub1', 'question': \"<Passage>: Patient: Pharmacists maintain that doctors should not be permitted to sell the medicine that they prescribe because doctors would then be tempted to prescribe unnecessary medicines in order to earn extra income. But pharmacists have a financial interest in having a monopoly on the sale of prescription medicines, so their objection to the sale of medicines by doctors cannot be taken seriously.\\n<Question>: What is the rationale for the patient's argument attempting to discredit a position by questioning the motives of the proponents of that position?\\nChoose the correct answer from the following options.\\n・ Lack of pharmacists' knowledge was not highlighted in the argument against putting them in charge of prescriptions\\n・ The patient does not invoke public opinion in their argument but instead highlights the incentive structure which introduces conflicts for pharmacists and doctors.\\n・ The patient correctly highlights that the pharmacists position also has a potential conflict of interest just like those of the doctors\\n・ None of the above choices\\n\\n\\nWhat is the correct answer?\", 'answer': 'The patient correctly highlights that the pharmacists position also has a potential conflict of interest just like those of the doctors', 'output': 'The answer is: The patient correctly highlights that the pharmacists position also has a potential conflict of interest just like those of the doctors'}\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    if 'sub' in i['qid']:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original_data_RULE에서 'qid'와 같은 'id_string'이 있는 row를 찾고\n",
    "# 그 row에서 row['others']['main_option_correctness']가 True/False를 Return한다\n",
    "qid = 'RULE_train_1_sub1'\n",
    "def get_main_option_correctness(qid):\n",
    "    row = original_data_RULE[original_data_RULE['id_string'] == qid]\n",
    "    selective = row['others'].item()['main_option_correctness']\n",
    "    return 'selective' if selective else 'eliminative'\n",
    "\n",
    "get_main_option_correctness(qid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
